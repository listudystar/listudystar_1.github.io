
# BAYES-新闻主题分类

##### 通过将新闻内容，jiaba分词，数据清洗（剔除停用词），采用TF-IDF提取关键词并得出词频向量


```python
import pandas as pd
import jieba # jieba分词器
```

### >> 1. 加载数据（数据表提取出标题，内容，URL等有用信息，去掉NAN值）

###### 数据源我们采用搜狗实验室提供的数据源：http://www.sogou.com/labs/resource/ca.php


```python
df_news = pd.read_table('./data/val.txt',names=['category','theme','URL','content'],encoding='utf-8') # encoding='utf-8'表示中文格式
df_news = df_news.dropna() # 去掉NAN值
df_news.head()
```

    d:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\t'.
      """Entry point for launching an IPython kernel.
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>category</th>
      <th>theme</th>
      <th>URL</th>
      <th>content</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>汽车</td>
      <td>新辉腾　４．２　Ｖ８　４座加长Ｉｎｄｉｖｉｄｕａｌ版２０１１款　最新报价</td>
      <td>http://auto.data.people.com.cn/model_15782/</td>
      <td>经销商　电话　试驾／订车Ｕ憬杭州滨江区江陵路１７８０号４００８－１１２２３３转５８６４＃保常...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>汽车</td>
      <td>９１８　Ｓｐｙｄｅｒ概念车</td>
      <td>http://auto.data.people.com.cn/prdview_165423....</td>
      <td>呼叫热线　４００８－１００－３００　服务邮箱　ｋｆ＠ｐｅｏｐｌｅｄａｉｌｙ．ｃｏｍ．ｃｎ</td>
    </tr>
    <tr>
      <th>2</th>
      <td>汽车</td>
      <td>日内瓦亮相　ＭＩＮＩ性能版／概念车－１．６Ｔ引擎</td>
      <td>http://auto.data.people.com.cn/news/story_5249...</td>
      <td>ＭＩＮＩ品牌在二月曾经公布了最新的ＭＩＮＩ新概念车Ｃｌｕｂｖａｎ效果图，不过现在在日内瓦车展...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>汽车</td>
      <td>清仓大甩卖一汽夏利Ｎ５威志Ｖ２低至３．３９万</td>
      <td>http://auto.data.people.com.cn/news/story_6144...</td>
      <td>清仓大甩卖！一汽夏利Ｎ５、威志Ｖ２低至３．３９万＝日，启新中国一汽强势推出一汽夏利Ｎ５、威志...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>汽车</td>
      <td>大众敞篷家族新成员　高尔夫敞篷版实拍</td>
      <td>http://auto.data.people.com.cn/news/story_5686...</td>
      <td>在今年３月的日内瓦车展上，我们见到了高尔夫家族的新成员，高尔夫敞篷版，这款全新敞篷车受到了众...</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_news.shape
```




    (5000, 4)



### >> 2. 使用jieba分词器对content进行处理

#### 1. 将content每条内容转换为list


```python
content = df_news.content.values.tolist()
print (content[1000])
```

    阿里巴巴集团昨日宣布，将在集团管理层面设立首席数据官岗位（Ｃｈｉｅｆ　Ｄａｔａ　Ｏｆｆｉｃｅｒ），阿里巴巴Ｂ２Ｂ公司ＣＥＯ陆兆禧将会出任上述职务，向集团ＣＥＯ马云直接汇报。＞菹ぃ和６月初的首席风险官职务任命相同，首席数据官亦为阿里巴巴集团在完成与雅虎股权谈判，推进“ｏｎｅ　ｃｏｍｐａｎｙ”目标后，在集团决策层面新增的管理岗位。０⒗锛团昨日表示，“变成一家真正意义上的数据公司”已是战略共识。记者刘夏
    

#### 2. 使用jieba分词


```python
content_S = []
for line in content:
    current_segment = jieba.lcut(line) # jieba分词
    if len(current_segment) > 1 and current_segment != '\r\n': #换行符
        content_S.append(current_segment)
```


```python
content_S[1000]
```




    ['阿里巴巴',
     '集团',
     '昨日',
     '宣布',
     '，',
     '将',
     '在',
     '集团',
     '管理',
     '层面',
     '设立',
     '首席',
     '数据',
     '官',
     '岗位',
     '（',
     'Ｃ',
     'ｈ',
     'ｉ',
     'ｅ',
     'ｆ',
     '\u3000',
     'Ｄ',
     'ａ',
     'ｔ',
     'ａ',
     '\u3000',
     'Ｏ',
     'ｆ',
     'ｆ',
     'ｉ',
     'ｃ',
     'ｅ',
     'ｒ',
     '）',
     '，',
     '阿里巴巴',
     'Ｂ',
     '２',
     'Ｂ',
     '公司',
     'Ｃ',
     'Ｅ',
     'Ｏ',
     '陆兆禧',
     '将',
     '会',
     '出任',
     '上述',
     '职务',
     '，',
     '向',
     '集团',
     'Ｃ',
     'Ｅ',
     'Ｏ',
     '马云',
     '直接',
     '汇报',
     '。',
     '＞',
     '菹',
     'ぃ',
     '和',
     '６',
     '月初',
     '的',
     '首席',
     '风险',
     '官',
     '职务',
     '任命',
     '相同',
     '，',
     '首席',
     '数据',
     '官亦为',
     '阿里巴巴',
     '集团',
     '在',
     '完成',
     '与',
     '雅虎',
     '股权',
     '谈判',
     '，',
     '推进',
     '“',
     'ｏ',
     'ｎ',
     'ｅ',
     '\u3000',
     'ｃ',
     'ｏ',
     'ｍ',
     'ｐ',
     'ａ',
     'ｎ',
     'ｙ',
     '”',
     '目标',
     '后',
     '，',
     '在',
     '集团',
     '决策',
     '层面',
     '新增',
     '的',
     '管理',
     '岗位',
     '。',
     '０',
     '⒗',
     '锛',
     '团',
     '昨日',
     '表示',
     '，',
     '“',
     '变成',
     '一家',
     '真正',
     '意义',
     '上',
     '的',
     '数据',
     '公司',
     '”',
     '已',
     '是',
     '战略',
     '共识',
     '。',
     '记者',
     '刘夏']



#### 2. 重构为dataframe结构


```python
df_content=pd.DataFrame({'content_S':content_S})
df_content.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>content_S</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[经销商, 　, 电话, 　, 试驾, ／, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[呼叫, 热线, 　, ４, ０, ０, ８, －, １, ０, ０, －, ３, ０, ０...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 在, 二月, 曾经, 公布, 了, 最新, 的, Ｍ, Ｉ...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[清仓, 大, 甩卖, ！, 一汽, 夏利, Ｎ, ５, 、, 威志, Ｖ, ２, 低至, ...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[在, 今年, ３, 月, 的, 日内瓦, 车展, 上, ，, 我们, 见到, 了, 高尔夫...</td>
    </tr>
  </tbody>
</table>
</div>



### >> 3. 数据清洗（利用停用词表进行筛选）


```python
# 停用词表
stopwords=pd.read_csv("stopwords.txt",index_col=False,sep="\t",quoting=3,names=['stopword'], encoding='utf-8')
stopwords.head(20) 
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>stopword</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>!</td>
    </tr>
    <tr>
      <th>1</th>
      <td>"</td>
    </tr>
    <tr>
      <th>2</th>
      <td>#</td>
    </tr>
    <tr>
      <th>3</th>
      <td>$</td>
    </tr>
    <tr>
      <th>4</th>
      <td>%</td>
    </tr>
    <tr>
      <th>5</th>
      <td>&amp;</td>
    </tr>
    <tr>
      <th>6</th>
      <td>'</td>
    </tr>
    <tr>
      <th>7</th>
      <td>(</td>
    </tr>
    <tr>
      <th>8</th>
      <td>)</td>
    </tr>
    <tr>
      <th>9</th>
      <td>*</td>
    </tr>
    <tr>
      <th>10</th>
      <td>+</td>
    </tr>
    <tr>
      <th>11</th>
      <td>,</td>
    </tr>
    <tr>
      <th>12</th>
      <td>-</td>
    </tr>
    <tr>
      <th>13</th>
      <td>--</td>
    </tr>
    <tr>
      <th>14</th>
      <td>.</td>
    </tr>
    <tr>
      <th>15</th>
      <td>..</td>
    </tr>
    <tr>
      <th>16</th>
      <td>...</td>
    </tr>
    <tr>
      <th>17</th>
      <td>......</td>
    </tr>
    <tr>
      <th>18</th>
      <td>...................</td>
    </tr>
    <tr>
      <th>19</th>
      <td>./</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 停用词匹配去掉停用词
def drop_stopwords(contents,stopwords):
    contents_clean = []
    all_words = []
    for line in contents:
        line_clean = []
        for word in line:
            if word in stopwords:
                continue
            line_clean.append(word)
            all_words.append(str(word))
        contents_clean.append(line_clean)
    return contents_clean,all_words
    #print (contents_clean)
        

contents = df_content.content_S.values.tolist()    
stopwords = stopwords.stopword.values.tolist()
contents_clean,all_words = drop_stopwords(contents,stopwords)
```


```python
df_content=pd.DataFrame({'contents_clean':contents_clean})
df_content.head() # drop_stopwords函数输出（清理完的DataFrame）
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>contents_clean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_all_words=pd.DataFrame({'all_words':all_words})
df_all_words.head() # drop_stopwords函数输出（清理完的所有的单词的DataFrame）
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>all_words</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>经销商</td>
    </tr>
    <tr>
      <th>1</th>
      <td>电话</td>
    </tr>
    <tr>
      <th>2</th>
      <td>试驾</td>
    </tr>
    <tr>
      <th>3</th>
      <td>订车</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Ｕ</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 统计词频（对数据有个大致的了解）
import numpy as np
words_count=df_all_words.groupby(by=['all_words'])['all_words'].agg({"count":np.size})
words_count=words_count.reset_index().sort_values(by=["count"],ascending=False)
words_count.head()
```

    d:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py:3: FutureWarning: using a dict on a Series for aggregation
    is deprecated and will be removed in a future version
      This is separate from the ipykernel package so we can avoid doing imports until
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>all_words</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4077</th>
      <td>中</td>
      <td>5199</td>
    </tr>
    <tr>
      <th>4209</th>
      <td>中国</td>
      <td>3115</td>
    </tr>
    <tr>
      <th>88255</th>
      <td>说</td>
      <td>3055</td>
    </tr>
    <tr>
      <th>104747</th>
      <td>Ｓ</td>
      <td>2646</td>
    </tr>
    <tr>
      <th>1373</th>
      <td>万</td>
      <td>2390</td>
    </tr>
  </tbody>
</table>
</div>



### >> 4. TF-IDF提取关键词


```python
import jieba.analyse
index = 1000
print (df_news['content'][index])
content_S_str = "".join(content_S[index])  
print ("  ".join(jieba.analyse.extract_tags(content_S_str, topK=5, withWeight=False))) 
# jieba.analyse.extract_tags方法是关键，topK是选取多少个关键词。
```

    阿里巴巴集团昨日宣布，将在集团管理层面设立首席数据官岗位（Ｃｈｉｅｆ　Ｄａｔａ　Ｏｆｆｉｃｅｒ），阿里巴巴Ｂ２Ｂ公司ＣＥＯ陆兆禧将会出任上述职务，向集团ＣＥＯ马云直接汇报。＞菹ぃ和６月初的首席风险官职务任命相同，首席数据官亦为阿里巴巴集团在完成与雅虎股权谈判，推进“ｏｎｅ　ｃｏｍｐａｎｙ”目标后，在集团决策层面新增的管理岗位。０⒗锛团昨日表示，“变成一家真正意义上的数据公司”已是战略共识。记者刘夏
    阿里巴巴  集团  首席  岗位  数据
    

### >> 5. BAYES分类


```python
# 加label的dataframe
df_train=pd.DataFrame({'contents_clean':contents_clean,'label':df_news['category']})
df_train.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>contents_clean</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4995</th>
      <td>[天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补...</td>
      <td>时尚</td>
    </tr>
    <tr>
      <th>4996</th>
      <td>[不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话...</td>
      <td>时尚</td>
    </tr>
    <tr>
      <th>4997</th>
      <td>[岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘...</td>
      <td>时尚</td>
    </tr>
    <tr>
      <th>4998</th>
      <td>[导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ...</td>
      <td>时尚</td>
    </tr>
    <tr>
      <th>4999</th>
      <td>[全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志...</td>
      <td>时尚</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_train.label.unique()
```




    array(['汽车', '财经', '科技', '健康', '体育', '教育', '文化', '军事', '娱乐', '时尚'],
          dtype=object)




```python
# 将label映射为数字
label_mapping = {"汽车": 1, "财经": 2, "科技": 3, "健康": 4, "体育":5, "教育": 6,"文化": 7,"军事": 8,"娱乐": 9,"时尚": 0}
df_train['label'] = df_train['label'].map(label_mapping)
df_train.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>contents_clean</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 数据集拆分
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df_train['contents_clean'].values, df_train['label'].values, test_size = 0.25, random_state=0)
```

### >> 6. 基于TF-IDF进行词频向量创建


```python
# 新闻信息转换成字符串形式
words = []
for line_index in range(len(x_train)):
    try:
        words.append(' '.join(x_train[line_index]))
    except:
        print (line_index,word_index)
# words[0]  
```


```python
# sklearn词频向量创建
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(analyzer='word', max_features=4000,  lowercase = False)
vectorizer.fit(words)

# naive_bayes训练
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(vectorizer.transform(words), y_train)
```

### >> 7. 基于TF-IDF进行词频向量创建

### >> 8. 测试


```python
# 新闻信息转换成字符串形式
words_1 = []
for line_index in range(len(x_test)):
    try:
        words_1.append(' '.join(x_test[line_index]))
    except:
        print (line_index,word_index)
words_1[0]  
```




    '经销商 电话 试驾 订车 ッ 魇 忻 窈 铰 罚常埃购 牛 矗 埃埃福 转 梗 万 俜 诼 喙 拇 笄 磐 东 米路 南华 港 对面 奥菲 汽车 家园 矗埃埃福 转 梗 万 Ｔ 顺腔 坪 蟮 佬 鲁导 熘 行亩 悦 纨 矗埃埃福 转 梗 万 Ｇ 嗟 菏校 常埃腹 道 号 城阳 工艺品 城旁 米 矗埃埃福 转 梗 万 Ｖ 楹 Ｊ 忻坊 西路 号 栋 斗门 白藤 湖 湖心 路 湖中 湖 巴士 旁 矗埃埃福 转 保保 万 门市 建设 三路 号 江门 侨乡 国际 茶叶 批发城 区 部门 矗埃埃福 转 梗 万 沤 市九瑞 大道 号 转 梗 万 菔 锌 图掖 蟮 溃 保福 焙 殴际 汽车城 二期 矗埃埃福 转 梗 万 媚市 高新区 国道 北 变电站 东 汽车 展厅 矗埃埃福 转 梗 万 啥际形 浜 钋 武科东 四路 号 转 梗 万 阒 菔 蟹禺区 市桥 街 迎宾 路段 东边 号 转 梗 万 Ｉ 峭肥辛 湖区 金鸿 公路 北侧 梗 万'




```python
# 测试
classifier.score(vectorizer.transform(words_1), y_test)
```




    0.8128


